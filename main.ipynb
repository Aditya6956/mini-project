{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "BUCKET_NAME = 'mini-project-bucket-text'\n",
    "\n",
    "def download_file(BUCKET_NAME):\n",
    "\n",
    "    objects = []\n",
    "    s3 = boto3.client('s3', aws_access_key_id = \" \", \n",
    "                    aws_secret_access_key = \" \")\n",
    "    \n",
    "    for key in s3.list_objects(Bucket=BUCKET_NAME)['Contents']:\n",
    "        objects.append(key['Key'])\n",
    "    \n",
    "    BUCKET_FILE_NAME = objects[-1]\n",
    "    s3.download_file(BUCKET_NAME, BUCKET_FILE_NAME, BUCKET_FILE_NAME)\n",
    "    return BUCKET_FILE_NAME\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_FILE_NAME = download_file(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def get_file_contents(LOCAL_FILE_NAME):\n",
    "   pdf_object = open(LOCAL_FILE_NAME, 'rb')\n",
    "   pdf_reader = PyPDF2.PdfFileReader(pdf_object)\n",
    "   num_pages = pdf_reader.numPages\n",
    "   text = []\n",
    "   for i in range(num_pages):\n",
    "      page_object = pdf_reader.getPage(i)\n",
    "      text += page_object.extract_text()\n",
    "   \n",
    "   pdf_object.close()\n",
    "   original_text = ''.join(text)\n",
    "   formatted_text = original_text.replace('\\\\n', '\\n')\n",
    "   return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24593"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = get_file_contents(LOCAL_FILE_NAME)\n",
    "# text = \"\"\"Federer won the Swiss Indoors last week by beating Romanian qualifier Marius Copil in\n",
    "# the final. The 37-year-old claimed his 99th ATP title and is hunting the century in\n",
    "# the French capital this week. Federer has been handed a difficult draw where could\n",
    "# could come across Kevin Anderson, Novak Djokovic and Rafael Nadal in the latter\n",
    "# rounds. But first the 20-time Grand Slam winner wants to train on the Paris Masters\n",
    "# court this afternoon before deciding whether to appear for his opening match against\n",
    "# either Milos Raonic or Jo-Wilfried Tsonga. \"On Monday, I am free and will look how I\n",
    "# feel,\" Federer said after winning the Swiss Indoors. \"On Tuesday I will fly to Paris\n",
    "# and train in the afternoon to be ready for my first match on Wednesday night. \"I felt\n",
    "# good all week and better every day. \"We also had the impression that at this stage it\n",
    "# might be better to play matches than to train. \"And as long as I fear no injury, I\n",
    "# play.\" Federer's success in Basel last week was the ninth time he has won his\n",
    "# hometown tournament. And he was delighted to be watched on by all of his family and\n",
    "# friends as he purchased 60 tickets for the final for those dearest to him. \"My\n",
    "# children, my parents, my sister and my team are all there,\" Federer added. \"It is\n",
    "# always very emotional for me to thank my team. And sometimes it tilts with the\n",
    "# emotions, sometimes I just stumble. \"It means the world to me. It makes me incredibly\n",
    "# happy to win my home tournament and make people happy here. \"I do not know if it's\n",
    "# maybe my last title, so today I try a lot more to absorb that and enjoy the moments\n",
    "# much more consciously. \"Maybe I should celebrate as if it were my last title. \"There\n",
    "# are very touching moments: seeing the ball children, the standing ovations, all the\n",
    "# familiar faces in the audience. Because it was not always easy in the last weeks.\"\"\"\n",
    "# with open('readme.txt', 'w', encoding='utf-8') as f:\n",
    "#     f.write(text)\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "\n",
    "stopwords = STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = punctuation + '\\n'\n",
    "\n",
    "word_frequencies={}\n",
    "for word in doc :\n",
    "    if word.text.lower() not in stopwords:\n",
    "        if word.text.lower() not in punctuation:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text] = 1\n",
    "            else:\n",
    "                word_frequencies[word.text] += 1\n",
    "\n",
    "# print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_frequency = max(word_frequencies.values())\n",
    "max_frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word]= word_frequencies[word]/max_frequency\n",
    "\n",
    "# print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = [sent for sent in doc.sents]\n",
    "# print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_score = {}\n",
    "for sent in sentence_tokens:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in word_frequencies.keys():\n",
    "            if sent not in sentence_score.keys():\n",
    "                sentence_score[sent]=word_frequencies[word.text.lower()]\n",
    "            else:\n",
    "                sentence_score[sent]+=word_frequencies[word.text.lower()]\n",
    "\n",
    "# sentence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  heapq import nlargest\n",
    "\n",
    "select_length = int(len(sentence_tokens)*0.3)\n",
    "select_length\n",
    "\n",
    "summary = nlargest(select_length,sentence_score,key=sentence_score.get)\n",
    "# summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = nlargest(select_length,sentence_score,key=sentence_score.get)\n",
    "# summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary =[word.text for word in summary]\n",
    "# print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \" \".join(final_summary)\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14838"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(summary)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from fpdf import FPDF\n",
    "\n",
    "def text_to_pdf(text, filename):\n",
    "    a4_width_mm = 210\n",
    "    pt_to_mm = 0.35\n",
    "    fontsize_pt = 10\n",
    "    fontsize_mm = fontsize_pt * pt_to_mm\n",
    "    margin_bottom_mm = 10\n",
    "    character_width_mm = 7 * pt_to_mm\n",
    "    width_text = a4_width_mm / character_width_mm\n",
    "\n",
    "    pdf = FPDF(orientation='P', unit='mm', format='A4')\n",
    "    pdf.set_auto_page_break(True, margin=margin_bottom_mm)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(family='Courier', size=fontsize_pt)\n",
    "    splitted = text.split('\\n')\n",
    "\n",
    "    for line in splitted:\n",
    "        lines = textwrap.wrap(line, width_text)\n",
    "\n",
    "        if len(lines) == 0:\n",
    "            pdf.ln()\n",
    "\n",
    "        for wrap in lines:\n",
    "            pdf.cell(0, fontsize_mm, wrap, ln=1)\n",
    "\n",
    "    pdf.output(filename, 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = 'test.txt'\n",
    "output_filename = 'summarized.pdf'\n",
    "file = open(input_filename, encoding='latin-1')\n",
    "text = file.read()\n",
    "file.close()\n",
    "text_to_pdf(text, output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8606adcc0ab6d2803bdc19d6c4291253b939f60e52ade4f028ecd3a76354300"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
